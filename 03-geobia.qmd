---
title: "Clasificaci√≥n de usos del suelo en R"
subtitle: "Geographic Object-Based Image Analysis (GEOBIA)"
author: "Adri√°n Cidre Gonz√°lez"
institute: "Universidad de C√≥rdoba"
format: 
  revealjs:
    preview-links: auto
    theme: [serif, 00_assets/styles/serif-style.scss]
    highlight-style: oblivion # Others: haddock, tango, kate
    transition: fade
    fig-align: center
title-slide-attributes: 
    data-background-image: 00_assets/figures/logo-geoforest.png, 00_assets/figures/logo-bosque-digital.png
    data-background-repeat: no-repeat
    data-background-position: 15% 90%, 85% 90%
    data-background-size: 20%, 20%
execute: 
  echo: false
slide-number: true
center-title-slide: true
transition: fade
transition-speed: slow
background-transition: fade
lightbox: true
fig-responsive: true
# filters: 
# webr:
#   packages: ["dplyr", "ggplot2", "gapminder", "forcats"]
# revealjs-plugins:
bibliography: references-geobia.bib
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: internal_packages
#| include: false
library(pacman)

p_load(correlation, ggtext, ggthemes, gt, reactable, link2GI, OTBsegm, patchwork, rpart, rpart.plot, sf, terra, tidymodels, tidyverse, yardstick, withr)

## variables
bg_color <- "#f0f1eb"

## load data
over_segment_sf <- read_sf("00_assets/data/segment_5_15_100.gpkg")

## load sample image
image_sr <- rast("00_assets/data/tenerife-rgb-sample.tiff")

## load training points
training_sf <- read_sf("00_assets/data/training-points-stats.gpkg")
annotated_data_tbl <- st_drop_geometry(training_sf)

## predictions
preds_tbl <- read_csv("00_assets/data/predictions.csv")

## metrics
metrics_tbl <- read_csv("00_assets/data/metrics.csv")

## ggplot theme
theme_set(
  theme_minimal() +
    theme(
      plot.title = element_text(
        hjust = .5,
        size  = rel(1.5)
      ),
      axis.title = element_text(size = rel(.7)),
      plot.caption = element_text(
        hjust  = .5,
        family = "Roboto",
        size   = rel(.6)
      ),
      plot.background = element_rect(fill = bg_color, color = NA)
    )
  )
```

{{< include 00_assets/partials/intro-acidre.qmd >}}

# Objetivos

-   Introducci√≥n a GEOBIA

-   Segmentaci√≥n

-   Toma de datos

-   Modelo de clasificaci√≥n

-   Implementaci√≥n del modelo

# Introducci√≥n a GEOBIA {.main-part .unnumbered}

-   Definici√≥n

-   Tendencias

-   Flujo de trabajo

-   GEOBIA vs Pixel-Based

{{< fa clock size=xl >}} 10min

## Introducci√≥n

GEOBIA es...

-   ...una metodolog√≠a para analizar im√°genes de alta resoluci√≥n

-   ...un m√©todo de clasificaci√≥n de im√°genes

-   ...superior a pixel-based analysis

-   ...similar a como los humanos interpretamos las im√°genes

> Una subdisciplina de *Geographic Information Science* comprometida a desarrollar m√©todos autom√°ticos de partici√≥n de im√°genes de teledetecci√≥n en objetos con un significado, y evaluar sus caracter√≠sticas en las escalas espacial, espectral y temporal, para generar nueva informaci√≥n geogr√°fica en un formato GIS.

## Tendencias GEOBIA

```{r}
#| label: publications_year
#| fig-height: 6

## create data from dimensions.ai
geobia_citations <- tibble(
  year = 2007:2024,
  citations = c(
    1, 11, 19, 51, 79, 113, 167, 207, 233,
    351, 299, 379, 410, 458, 438, 456, 425, 454
  )
)

## visualize
ggplot(geobia_citations, aes(x = year, y = citations)) +
  geom_line(color = "#2C3E50", size = 1.2) +
  geom_point(color = "#E74C3C", size = 2) +
  # theme_minimal(base_family = "Passion One") +
  ggthemes::theme_base(base_family = "Passion One", base_size = 12) +
  scale_x_continuous(breaks = seq(2007, 2024, 1)) +
  labs(
    title   = "Citas por a√±o de 'GEOBIA'",
    x       = NULL,
    y       = "N√∫mero de citas",
    caption = "Fuente: propia | Datos: Dimensions.ai"
  ) +
  theme(
    plot.title = element_text(
      hjust = .5,
      size  = rel(1.5)
    ),
    axis.title = element_text(size = rel(.7)),
    plot.caption = element_text(
      hjust  = .5,
      family = "Roboto",
      size   = rel(.6)
    ),
    plot.background = element_rect(fill = "#f0f1eb", color = NA)
  )

```

## Workflow

![](00_assets/figures/03-geobia/geobia-workflow.png){fig-align="center"}

## GEOBIA vs Pixel-Based

-   GEOBIA: clasifica objetos

-   Pixel-based: clasifica p√≠xeles

![](00_assets/figures/03-geobia/meme-geobia-vs-pixel-nobg.png){fig-align="center" width="669"}

##  {.background-no-title}

::::: columns
::: {.column width="50%"}
![](00_assets/figures/03-geobia/step-03-train-areas.png){fig-align="center" width="424"}
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/train-areas-pixel.png){fig-align="center" width="424"}
:::
:::::

::::: columns
::: {.column width="50%"}
![](00_assets/figures/03-geobia/zoom-geobia-segment.png){fig-align="center" width="422"}
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/zoom-pixel.png){fig-align="center" width="488"}
:::
:::::

## Interpretaci√≥n de una imagen

Elementos √∫tiles para la interpretaci√≥n de una imagen a√©rea:

::::: columns
::: {.column width="20%"}
-   Forma

-   Tama√±o

-   Tonos

-   Textura

-   Patr√≥n

-   Sombras

-   Asociaci√≥n

-   Lugar
:::

::: {.column width="80%"}
![](00_assets/figures/03-geobia/santa-cruz.png){fig-align="right"}
:::
:::::

## Ventajas GEOBIA

![](00_assets/figures/03-geobia/meme-pepe-sunglasses.png){fig-align="center" width="533"}

::: incremental
-   Similar a c√≥mo los humanos interpretamos los objetos

-   Reducci√≥n de la carga computacional del clasificador

-   Extracci√≥n de caracter√≠sticas √∫tiles (forma, textura...)

-   Modelos de segmentaci√≥n (meanshift, watershed, SAM)
:::

## Desventajas GEOBIA

![](00_assets/figures/03-geobia/meme-pepe-sad.png){fig-align="center" width="313"}

::: incremental
-   Problemas computacionales con grandes im√°genes

-   Segmentaci√≥n sin soluci√≥n √∫nica

-   Evaluaci√≥n de la segmentaci√≥n
:::

# Segmentaci√≥n {.main-part .unnumbered}

-   Objetivos segmentaci√≥n

-   Algoritmos segmentaci√≥n

-   Software

-   Pr√°ctica 1

{{< fa clock size=xl >}} 30min

## Objetivos segmentaci√≥n

::::: columns
::: {.column width="50%"}
-   Generar √°reas de un tama√±o razonable

-   Evitar segmentaci√≥n excesiva o insuficiente

-   Generar √°reas representativas de cada clase
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/rgb-good-segment.png){fig-align="center" width="405"}
:::
:::::

::: callout-important
## Importante

Definir las clases que queremos predecir con el modelo
:::

## Ejemplo {.special-size}

::::: {.columns style="display: flex !important; height: 90%;"}
::: {.column width="40%" style="display: flex; justify-content: center; align-items: center;"}
```{r}
image_sr
```
:::

::: {.column width="60%"}
![](00_assets/figures/03-geobia/rgb-image.png){fig-align="center"}
:::
:::::

## Sobresegmentaci√≥n

::::: {.columns style="display: flex !important; height: 90%;"}
::: {.column width="50%" style="display: flex; justify-content: center; align-items: center;"}
![](00_assets/figures/03-geobia/code-over-segment.png)
:::

::: {.column width="60%"}
![](00_assets/figures/03-geobia/rgb-over-segment.png){fig-align="center"}
:::
:::::

## Subsegmentaci√≥n

::::: {.columns style="display: flex !important; height: 90%;"}
::: {.column width="50%" style="display: flex; justify-content: center; align-items: center;"}
![](00_assets/figures/03-geobia/code-under-segment.png)
:::

::: {.column width="60%"}
![](00_assets/figures/03-geobia/rgb-under-segment.png){fig-align="center"}
:::
:::::

## Segmentaci√≥n correcta

::::: {.columns style="display: flex !important; height: 90%;"}
::: {.column width="50%" style="display: flex; justify-content: center; align-items: center;"}
![](00_assets/figures/03-geobia/code-good-segment.png)
:::

::: {.column width="60%"}
![](00_assets/figures/03-geobia/rgb-good-segment.png){fig-align="center"}
:::
:::::

## Algoritmos segmentaci√≥n

-   Threshold (umbrales): se utilizan umbrales para clasificar los p√≠xeles

-   **Region-based (similitud): crecimiento de regiones, separaci√≥n-uni√≥n**

-   Boundary-based (discontinuidad): partici√≥n de una imagen basado en cambios en los valores digitales (borde-no borde).

-   Algoritmos de *Deep Learning*: SAM, Mask R-CNN.

## Threshold

```{r}
#| label: load_cabra
cabras_sr <- rast("00_assets/figures/03-geobia/cabras-barco.jpg") |> flip()
cabras_th_sr <- ifel(cabras_sr > 150, 1, 0)
```

::::: columns
::: {.column width="50%"}
![](00_assets/figures/03-geobia/cabras-barco.jpg)
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/cabras-segment.png)
:::
:::::

```{r}
#| label: fig-hist-cabra
#| fig-height: 2.3
as_tibble(cabras_sr[[1]]) |> 
  ggplot(aes(`cabras-barco_1`)) +
  geom_histogram(color = "snow", fill = "#0073C2FF") +
  geom_vline(xintercept = 150, color = "red", lwd = 1) +
  ggthemes::theme_base(base_size = 8) +
  scale_y_continuous(expand = expansion(0)) +
  labs(x = NULL, y = NULL) +
  theme(
    plot.background = element_rect(fill = bg_color, color = NA)
  )
```

## Region-based {.special-size}

::::: columns
::: {.column width="50%"}
Watershed:

-   Interpreta la imagen como una topograf√≠a y simula cu√°nto se llenar√≠a de agua
-   Las l√≠neas divisorias son los bordes de la segmentaci√≥n
:::

::: {.column width="50%"}
![Fuente: https://datahacker.rs/007-opencv-projects-image-segmentation-with-watershed-algorithm/](00_assets/figures/03-geobia/algo-watershed.jpg){fig-align="center" width="435"}
:::
:::::

## Region based {.special-size}

::::: columns
::: {.column width="50%"}
Mean-shift:

-   Agrupa p√≠xeles en funci√≥n de la similitud

-   Par√°metros: *spatialr, ranger, minsize*

Pasos:

-   Para cada p√≠xel, se define una ventana de b√∫squeda en el espacio espectral y espacial.

-   Se calcula la media de todos los p√≠xeles dentro de esa ventana (en color y posici√≥n).

-   Se actualiza la posici√≥n del centro hacia esa media ‚Üí repite hasta converger.

-   Todos los p√≠xeles que convergen al mismo modo son etiquetados como una regi√≥n.
:::

::: {.column width="50%"}
![Fuente: https://ailephant.com/how-to-program-mean-shift/](00_assets/figures/03-geobia/algo-meanshift.png){fig-align="center" width="432"}
:::
:::::

## Segment Anything Model (SAM)

-   Modelo pre-entrenado de Meta AI

-   Modelo de *Deep Learning* encoder-decoder

-   Disponible en Python

## Segment Anything Model (SAM)

-   Modelo pre-entrenado de Meta AI

-   Modelo de *Deep Learning* encoder-decoder

-   Disponible en Python

![](00_assets/figures/03-geobia/cabras-masks.png){fig-align="center" width="534"}

## Software

::::: columns
::: {.column width="70%"}
Orfeo Toolbox

-   Algoritmos para teledetecci√≥n

-   Software libre

-   QGIS, R, Python, CMD, C++

-   Segmentaci√≥n:

    -   Mean-shift

    -   Watershed

    -   Large Scale Meanshift
:::

::: {.column width="30%"}
![](00_assets/figures/03-geobia/logo-orfeo.jpg){fig-align="center"}

![](00_assets/figures/03-geobia/logo-otbsegm.png){fig-align="center" width="200"}
:::
:::::

## üí™ Pr√°ctica 1 {.exercise}

-   Descarga Orfeo Toolbox

-   Segmentaci√≥n en R. Paquetes a utilizar:

    -   `arrow/geoarrow`: formato GeoParquet

    -   `link2GI`: acceso a aplicaciones de Orfeo Toolbox

    -   `mapview`: visualizaci√≥n interactiva

    -   `OTBsegm`: *wrapper* de las aplicaciones de segmentaci√≥n de OTB

    -   `sf`: manejo de datos vectoriales

    -   `terra`: manejo de datos r√°ster

    -   `tidyverse`: paquetes para an√°lisis y visualizaci√≥n de datos

# Toma de datos {.main-part .unnumbered}

-   Objetivo

-   Toma de datos

-   Estad√≠sticas zonales

-   Pr√°ctica 2

{{< fa clock size=xl >}} 20min

## Objetivo

![](00_assets/figures/03-geobia/geobia-workflow.png){fig-align="center"}

## Objetivo

![](00_assets/figures/03-geobia/qgis-puntos.png){fig-align="center"}

## Estad√≠sticas zonales

::::: {.columns style="display: flex !important; height: 90%;"}
::: {.column width="50%" style="display: flex; justify-content: center; align-items: center;"}
![](00_assets/figures/03-geobia/code-exactextract.png)
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/fstats-mapview.png){fig-align="center" width="488"}
:::
:::::

## üí™ Pr√°ctica 2 {.exercise}

-   Toma de datos en QGIS

-   Extracci√≥n estad√≠sticas. Paquetes a utilizar

    -   `arrow/geoarrow`: formato GeoParquet

    -   `exactextractr`: estad√≠sticas zonales

    -   `mapview`: visualizaci√≥n interactiva

    -   `sf`: manejo de datos vectoriales

    -   `terra`: manejo de datos r√°ster

    -   `tidyverse`: paquetes para an√°lisis y visualizaci√≥n de datos

# Modelo de clasificaci√≥n {.main-part .unnumbered}

-   Introducci√≥n

-   Algoritmos clasificaci√≥n supervisada

-   Pasos modelado

-   Workflow tidymodels

-   Pr√°ctica 3

{{< fa clock size=xl >}} 60min

## Introducci√≥n {.special-size}

Conceptos de *machine learning*:

-   **Clasificaci√≥n**: el objetivo del modelo es predecir una variable categ√≥rica (etiqueta, clase) a partir de un conjunto de variables. Para ello se entrena un modelo con datos ya clasificados, que aprende los patrones y los aplica a datos nuevos.

-   **Regresi√≥n**: el objetivo del modelo es predecir una variable continua a partir de un conjunto de variables.

```{r}
reactable(
  iris,
  defaultColDef = colDef(
    header = function(value) gsub(".", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center",
    minWidth = 70,
    headerStyle = list(background = "#f7f7f8")
  ),
  columns = list(
    Species = colDef(minWidth = 140)  # overrides the default
  ),
  bordered = TRUE,
  highlight = TRUE
)
```

## Introducci√≥n {.special-size}

Conceptos de *machine learning*:

::::: columns
::: {.column width="50%"}
-   **Aprendizaje no supervisado**: algoritmos que trabajan sin etiquetas (clustering, segmentaci√≥n *mean-shift,* PCA)

-   **Aprendizaje supervisado**: algoritmos que aprenden de datos etiquetados (clasificaci√≥n, regresi√≥n, CNN)
:::

::: {.column width="50%"}
![Fuente: @morimoto2021](00_assets/figures/03-geobia/supervised-non-supervised.jpg){fig-cap-location="margin" fig-align="center" width="396"}
:::
:::::

## Introducci√≥n {.special-size}

Conceptos de *machine learning*:

-   **Clasificaci√≥n no supervisada** (*clustering*): algoritmos de clasificaci√≥n que trabajan sin etiquetas. Agrupan los datos seg√∫n su similitud

-   **Clasificaci√≥n supervisada**: algoritmos de clasificaci√≥n que aprenden de datos etiquetados

```{r}
tibble(
  `Propiedad` = c(
    "Datos de entrenamiento", 
    "Precisi√≥n", 
    "Output", 
    "Algoritmos", 
    "Intervenci√≥n", 
    "Casos de uso"
  ),
  `Clasificaci√≥n supervisada` = c(
    "Necesita datos con clases conocidas",
    "Generalmente mayor",
    "Clases predefinidas",
    "Random forest, SVM, XGBoost...",
    "Se necesita etiquetar datos y entrenar el algoritmo",
    "Clasificaci√≥n (usos del suelo, modelos combustible..)"
  ),
  `Clasificaci√≥n no supervisada` = c(
    "No necesita datos con clases",
    "Generalmente menor",
    "Clusters/grupos - post-interpretaci√≥n",
    "K-Means, ISODATA, Hierachical clustering",
    "M√≠nima",
    "Identificaci√≥n de patrones, exploraci√≥n de datos"
  ) 
) |> 
  gt() |> 
  opt_stylize(2) |> 
  gt::opt_table_font(size = "140%")
```

## Algoritmos clasificaci√≥n supervisada

-   Regresi√≥n log√≠stica

-   **K-Nearest Neighbors**

-   Support Vector Machine

-   Naive Bayes

-   Decision tree

-   **Random Forest**

-   XGBoost

::: callout-caution
## Atenci√≥n

Un algoritmo **no** es un modelo *sensu stricto*
:::

## Pasos modelado

![](00_assets/figures/03-geobia/machine-learning-steps.png){fig-align="center" width="2440"}

## Paso 1 - Obtener datos

![](00_assets/figures/03-geobia/qgis-puntos.png){fig-align="center"}

## Paso 2 - Preparar datos

```{r}
reactable(
  st_drop_geometry(training_sf),
  defaultColDef = colDef(
    header = function(value) gsub(".", " ", value, fixed = TRUE),
    align = "center",
    minWidth = 70,
    headerStyle = list(background = "#f7f7f8"),
    format = colFormat(digits = 1)
  ),
  columns = list(
    class = colDef(minWidth = 100)
  ),
  bordered = TRUE,
  highlight = TRUE,
  style = list(fontSize = "1rem")
)
```

## Pasos 3, 4 y 5 - Modelado

![](00_assets/figures/03-geobia/model-train.png){fig-align="center"}

## Tidymodels

Colecci√≥n de paquetes de R para *machine learning*

![](00_assets/figures/03-geobia/package-tidymodels.png){fig-align="center"}

## Separaci√≥n de datos

-   **Datos de entrenamiento**: el modelo aprende de estos datos (70-80%)

-   **Datos de prueba**: el modelo se eval√∫a en estos datos (20-30%)

```{r}
#| label: create_models
# Set seed for reproducibility
set.seed(666)

# Generate data: cubic relationship
n <- 50
x <- seq(-2, 2, length.out = n)
y <- x^3 - x + rnorm(n, sd = .8)  # Cubic curve + noise
data <- data.frame(x = x, y = y)

# data split
splits    <- initial_split(data, prop = .8)
train_tbl <- training(splits)
test_tbl  <- testing(splits)

# Create models
model_underfit <- lm(y ~ x, data = train_tbl)     
model_goodfit  <- lm(y ~ poly(x, 3), data = train_tbl)  
# model_overfit  <- lm(y ~ poly(x, 15), data = train_tbl) 
model_overfit  <- loess(y ~ x, data = train_tbl, span = .16)

# Create predictions
train_tbl <- train_tbl %>%
    mutate(
        pred_underfit = predict(model_underfit, newdata = train_tbl),
        pred_goodfit = predict(model_goodfit, newdata = train_tbl),
        pred_overfit = predict(model_overfit, newdata = train_tbl)
    )

# Create a function to plot
plot_fit <- function(pred_column, title, train_r2) {
    ggplot(train_tbl, aes(x = x, y = y)) +
        geom_point(color = "black", alpha = 0.6, size = 2) +
        geom_line(aes(y = !!sym(pred_column)), color = "blue", size = .8) +
        labs(title = title, x = "x", y = "y") +
        theme_bw() +
        theme(
            plot.title = element_text(
                family = "Merriweather",
                hjust  = .5,
                size   = rel(1.5),
                face   = "bold"
            )
        ) +
        annotate(
          "richtext",
          label = glue::glue("R^2^ = {train_r2}"),
          x = 1,
          y = -5
        )
}

# Get r2
underfit.r2 <- round(summary(model_underfit)$r.squared, 2)
overfit.r2 <- round(yardstick::rsq(train_tbl, y, pred_overfit)$.estimate, 2)
# overfit.r2 <- round(summary(model_overfit)$r.squared, 2)
goodfit.r2 <- round(summary(model_goodfit)$r.squared, 2)
```

```{r}
# Plot each case
p1 <- plot_fit("pred_underfit", "Underfitting", underfit.r2)
p2 <- plot_fit("pred_goodfit", "Good Fit", goodfit.r2)
p3 <- plot_fit("pred_overfit", "Overfitting", overfit.r2)

# Display them
p1 + p2 + p3
```

## Separaci√≥n de datos

-   **Datos de entrenamiento**: el modelo aprende de estos datos (70-80%)

-   **Datos de prueba**: el modelo se eval√∫a en estos datos (20-30%)

```{r}
test_met_tbl <- test_tbl |> 
  mutate(
    goodfit  = predict(model_goodfit, test_tbl),
    overfit  = predict(model_overfit, test_tbl),
    underfit = predict(model_underfit, test_tbl)
  )

goodfit.r2 <- round(yardstick::rsq(test_met_tbl, y, goodfit)$.estimate, 2)
overfit.r2 <- round(yardstick::rsq(test_met_tbl, y, overfit)$.estimate, 2)
underfit.r2 <- round(yardstick::rsq(test_met_tbl, y, underfit)$.estimate, 2)

# Plot each case
p1 <- plot_fit("pred_underfit", "Underfitting", underfit.r2) + 
    geom_point(data = test_tbl, color = "red", size = 3) 
p2 <- plot_fit("pred_goodfit", "Good Fit", goodfit.r2) + 
    geom_point(data = test_tbl, color = "red", size = 3)
p3 <- plot_fit("pred_overfit", "Overfitting", overfit.r2) + 
    geom_point(data = test_tbl, color = "red", size = 3)

# Display them
p1 + p2 + p3
```

## Separaci√≥n de datos

-   **Datos de entrenamiento**: el modelo aprende de estos datos (70-80%)

-   **Datos de prueba**: el modelo se eval√∫a en estos datos (20-30%)

```{r}
test_met_tbl <- test_tbl |> 
  mutate(
    goodfit  = predict(model_goodfit, test_tbl),
    overfit  = predict(model_overfit, test_tbl),
    underfit = predict(model_underfit, test_tbl)
  )

goodfit.r2 <- round(yardstick::rsq(test_met_tbl, y, goodfit)$.estimate, 2)
overfit.r2 <- round(yardstick::rsq(test_met_tbl, y, overfit)$.estimate, 2)
underfit.r2 <- round(yardstick::rsq(test_met_tbl, y, underfit)$.estimate, 2)

# Plot each case
p1 <- plot_fit("pred_underfit", "Underfitting", underfit.r2) + 
    geom_linerange(
      aes(x = x, ymin = y, ymax = underfit),
      data = test_met_tbl
    ) +
    geom_point(data = test_tbl, color = "red", size = 3) 

p2 <- plot_fit("pred_goodfit", "Good Fit", goodfit.r2) + 
    geom_linerange(
      aes(x = x, ymin = y, ymax = goodfit),
      data = test_met_tbl
    ) +
    geom_point(data = test_tbl, color = "red", size = 3)

p3 <- plot_fit("pred_overfit", "Overfitting", overfit.r2) + 
  geom_linerange(
    aes(x = x, ymin = overfit, ymax = y),
    data = test_met_tbl
  ) +
  geom_point(data = test_tbl, color = "red", size = 3)

# Display them
p1 + p2 + p3
```

## Sobreajuste

![](00_assets/figures/03-geobia/overfitting-bed.jpg){fig-align="center"}

## Separaci√≥n de datos

::::: columns
::: {.column width="50%"}
Paquete `rsample`:
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/package-rsample.png){fig-align="center" width="87"}
:::
:::::

```{r}
#| echo: true
set.seed(137)
splits    <- initial_split(annotated_data_tbl, prop = .8, strata = class)
train_tbl <- training(splits)
test_tbl  <- testing(splits)
```

::::: columns
::: {.column width="50%"}
```{r}
#| echo: true
train_tbl
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
test_tbl
```
:::
:::::

## Feature engineering

-   **Selecci√≥n de variables**: selecci√≥n de variables relevantes {{< fa check >}}

-   **Transformaci√≥n de variables**:

    -   One Hot Encoding: variables categ√≥ricas {{< fa xmark >}}

    -   Normalizaci√≥n/estandarizaci√≥n: variables continuas {{< fa check >}}

-   **Creaci√≥n de variables**: ratios, polinomios, interacciones... {{< fa xmark >}}

-   **Reducci√≥n de la dimensionalidad**: PCA, UMAP... {{< fa xmark >}}

::: {.fragment .fade-up}
Mejores variables --\> mejores modelos
:::

:::: {.fragment .fade-up}
::: callout-tip
En la mayor√≠a de los problemas, *feature engineering* tiene un mayor impacto en el modelo que elegir un algoritmo *cool*.
:::
::::

## Feature engineering {.special-size}

Selecci√≥n de variables:

-   Algunos algoritmos son sensibles a variables irrelevantes (KNN)

-   Algunos algoritmos son sensibles a variables correlacionadas (KNN)

-   Explorar datos y relaciones de variables

```{r}
training_sf |> 
  st_drop_geometry() |> 
  correlation() |> 
  summary() |> 
  plot() +
    ggtitle("Matriz de Correlaci√≥n")
```

## Feature engineering

Transformaci√≥n de variables continuas:

-   Algunos algoritmos son sensibles a diferencias de magnitud

-   Algoritmos basados en distancias (KNN, SVM, ANN)

-   Normalizar (0, 1) / estandarizar ($\mu = 0 \; \ \sigma = 1$)

:::: {.fragment .fade-up}
::: callout-tip
Normalizar no tiene relevancia en algoritmos como *Random Forest* o *XGBoost*
:::
::::

## Feature engineering

::::: columns
::: {.column width="50%"}
Paquete `recipes`:
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/package-recipes.png){fig-align="center" width="87"}
:::
:::::

```{r}
#| echo: true
base_rec <- recipe(class ~ ., data = train_tbl) |> 
  step_zv(all_predictors()) |>  
  step_lincomb(all_predictors())
```

```{r}
#| echo: true
base_rec |> prep() |> juice()
```

## Algoritmos

**√Årboles de decisi√≥n**: en este algoritmo se basan *random forest* y *xgboost*

```{r}
dt_iris <- rpart(Species ~ ., iris)
rpart.plot(dt_iris, 5)
```

## Algoritmos

**Random Forest**: conjunto de √°rboles de decisi√≥n

![Fuente: https://www.turing.com/kb/random-forest-algorithm](00_assets/figures/03-geobia/algo-random-forest-removebg-preview.png){fig-align="center"}

## Algoritmos

**K-Nearest Neighbor (KNN)**: clasifica en la clase m√°s similar seg√∫n la clase de los vecinos m√°s cercanos

![Fuente: https://intuitivetutorial.com/2023/04/07/k-nearest-neighbors-algorithm/](00_assets/figures/03-geobia/algo-knn.png){fig-align="center"}

## Algoritmos

::::: columns
::: {.column width="50%"}
Paquete `parsnip`:
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/package-parsnip.png){fig-align="center" width="87"}
:::
:::::

```{r}
#| echo: true
rf_spec <- rand_forest(
    mode  = "classification",
    trees = 500,
    mtry  = tune(),
    min_n = tune()
  ) |> 
  set_engine(
    engine     = "ranger",
    importance = "permutation"
  )
```

## Remuestreo {.special-size}

**Resampling**: consiste en separar los datos de entrenamiento en dos grupos:

-   An√°lisis: datos de entrenamiento del modelo

-   Evaluaci√≥n: datos que se utilizan para evaluar el modelo

V-fold Cross-Validation: el m√°s com√∫n.

![Fuente: scikit-learn](00_assets/figures/03-geobia/grid_search_cross_validation.png){fig-align="center" width="650"}

## Remuestreo

![](00_assets/figures/03-geobia/resampling.svg){fig-align="center"}

## Remuestreo

::::: columns
::: {.column width="50%"}
Paquete `rsample`:
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/package-rsample.png){fig-align="center" width="87"}
:::
:::::

```{r}
#| echo: true
set.seed(137)
folds <- vfold_cv(train_tbl, strata = class, v = 5)
```

```{r}
folds
```

## Hyperparameter tuning

-   Los hiperpar√°metros no son aprendidos por el modelo

-   Deben probarse distintos valores para elegir los m√°s adecuados

-   Grid search: prueba un n√∫mero determinado de valores

-   Se prueban durante el entrenamiento del modelo. Una vez entrenado, se eligen los mejores hiperpar√°metros

-   Evita el sobreajuste

![](00_assets/figures/03-geobia/package-tune.png){fig-align="center" width="130"}

## Hiperpar√°metros KNN

K: n√∫mero de vecinos

::::: columns
::: {.column width="50%"}
##### Con tuning

```{r}
#| echo: true
knn_spec <- nearest_neighbor(
  mode      = "classification",
  neighbors = tune()
)
```
:::

::: {.column width="50%"}
##### Sin tuning

```{r}
#| echo: true
knn_spec <- nearest_neighbor(
  mode      = "classification",
  neighbors = 5
)
```
:::
:::::

![](00_assets/figures/03-geobia/algo-knn.png){fig-align="center"}

## Hiperpar√°metros Random Forest {.special-size}

-   **mtry**: n√∫mero de variables seleccionadas aleatoriamente

-   **min_n**: n√∫mero de observaciones m√≠nima para que un nodo se vuelva a separar

-   **trees**: n√∫mero de √°rboles de decisi√≥n

![](00_assets/figures/03-geobia/algo-random-forest.png){fig-align="center" width="470"}

Variables: color, forma, altura, anchura, peso, volumen, contenido de az√∫car, di√°metro

## Tunear en tidymodels {.special-size}

##### Sin tuning

```{r}
#| eval: false
#| echo: true
model_fit <- fit_resamples(
  model_wflw,
  resamples = folds,
  metrics   = metric_set(accuracy, f_meas)
)
```

##### Tuning 1 modelo

```{r}
#| eval: false
#| echo: true
model_fit <- tune_grid(
  model_wflw,
  resamples = folds,
  grid      = 20,
  metrics   = metric_set(accuracy, f_meas)
)
```

##### Tuning varios modelos

```{r}
#| eval: false
#| echo: true
models_fit <- workflow_map(
  model_wflw,
  fn        = "tune_grid",
  resamples = folds,
  grid      = 20, 
  metrics   = metric_set(accuracy, f_meas)
)
```

## Evaluaci√≥n del modelo

::::: columns
::: {.column width="50%"}
Paquete `yardstick`: m√©tricas que comparan valores reales con valores predichos.
:::

::: {.column width="50%"}
![](00_assets/figures/03-geobia/package-yardstick.png){fig-align="center" width="87"}
:::
:::::

```{r}
reactable(
  preds_tbl |> select(.pred_class:class),
  defaultColDef = colDef(
    header = function(value) gsub(".", " ", value, fixed = TRUE),
    align = "center",
    minWidth = 70,
    headerStyle = list(background = "#f7f7f8"),
    format = colFormat(digits = 3)
  ),
  columns = list(
    class = colDef(minWidth = 100)
  ),
  bordered = TRUE,
  highlight = TRUE,
  style = list(fontSize = "1rem")
)
```

## Evaluaci√≥n del modelo {.special-size}

-   Datos de entrenamiento:

    -   Datos de an√°lisis: no se utilizan las m√©tricas

    -   Datos de evaluaci√≥n: para elegir hiperpar√°metros (promedio de los folds)

-   Datos de prueba: para evaluar el modelo

![](00_assets/figures/03-geobia/resampling.svg){fig-align="center" width="659"}

## Pasos

-   5-fold cross-validation -\> 5 m√©tricas de evaluaci√≥n

-   Grilla de 20 hiperpar√°metros -\> 5 m√©tricas de evaluaci√≥n por conjunto

-   Se promedian las m√©tricas para cada conjunto -\> 20 m√©tricas promediadas

-   Se elige el mejor

```{r}
reactable(
  metrics_tbl |> select(-.config),
  defaultColDef = colDef(
    header = function(value) gsub(".", " ", value, fixed = TRUE),
    align = "center",
    minWidth = 70,
    headerStyle = list(background = "#f7f7f8"),
    format = colFormat(digits = 0)
  ),
  columns = list(
    class = colDef(minWidth = 100),
    std_err = colDef(format = colFormat(digits = 3)),
    mean = colDef(format = colFormat(digits = 3))
  ),
  bordered = TRUE,
  highlight = TRUE,
  style = list(fontSize = "1rem")
)
```

## Evaluaci√≥n {.special-size}

√öltimo ajuste:

-   Ajusta el modelo a TODOS los datos de entrenamiento

-   Eval√∫a en los datos de prueba (**reportar este valor**)

```{r}
#| eval: false
test_res <- last_fit(
  best_wflow,
  split   = splits,
  metrics = metric_set(accuracy, f_meas)
)
```

![](00_assets/figures/03-geobia/confmat.png){fig-align="center"}

## M√©tricas de evaluaci√≥n {.special-size}

-   *Accuracy* (precisi√≥n): proporci√≥n de aciertos. No usar con clases desbalanceadas.

-   F1 Score: balance √≥ptimo entre *precision* y *recall*. Usar con clases desbalanceadas.

![](00_assets/figures/03-geobia/eval-metrics.png){fig-align="center" width="640"}

## Caso pr√°ctico {.special-size}

::::: columns
::: {.column width="50%"}
Evaluaci√≥n:

-   800 casos negativos de tumor

-   20 casos positivos de tumor
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
conf_matrix <- data.frame(
  Real = factor(rep(c("Positivo", "Negativo"), each = 2), levels = c("Positivo", "Negativo")),
  Predicho = factor(rep(c("Negativo", "Positivo"), 2), levels = c("Negativo", "Positivo")),
  Casos = c(19, 1, 800, 0)
)

# Visualizaci√≥n
ggplot(conf_matrix, aes(x = Predicho, y = Real, fill = Casos)) +
  geom_tile(color = "white", show.legend = FALSE) +
  geom_text(aes(label = Casos), color = "black", size = 6) +
  scale_fill_gradient(low = "gray70", high = "gray50") +
  labs(
       x = "Predicci√≥n del Modelo",
       y = "Valor Real") +
  theme_minimal(base_size = 14) +
    scale_x_discrete(expand = expansion(0)) +
    scale_y_discrete(expand = expansion(0)) 

```
:::
:::::

$$
Accuracy = \frac{800 + 1}{800 + 19 + 1 + 0} = 0.977
$$

$$
F1 = \frac{1 \cdot 0.05}{1 + 0.05} = 0.095
$$

## Recapitulaci√≥n

## üí™ Pr√°ctica 3 {.exercise}

-   Generar modelo con tidymodels

-   Evaluar modelo

    -   `corrplot`: gr√°fico de correlaci√≥n

    -   `kknn`: algoritmo KNN en *tidymodels*

    -   `ranger`: algoritmo Random Forest en *tidymodels*

    -   `sf`: manejo de datos vectoriales

    -   `terra`: manejo de datos r√°ster

    -   `tidymodels`: paquetes para modelado

    -   `tidyverse`: paquetes para an√°lisis y visualizaci√≥n de datos

    -   `vip`: c√°lculo importancia de variables

## Referencias
